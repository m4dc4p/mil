\documentclass[12pt]{report}
%include polycode.fmt
\include{preamble}
\begin{document}
\include{document.preamble}

\chapter{Functional Languages}
\label{ref_chapter_languages}

%% \emph{Brief definitino of ``functional languages.'' Introduce our
%%   lambda calculus variant. Shows a compilation scheme from the variant
%%   to Caffeine's RTL as an example}

\section{The \LamA}
%%\emph{Why is it important}

In mathematics, a ``function'' takes one or more arguments and
produces some value. ``Functional programming languages'' are based
around the same idea -- the definition and evaluation of
functions. They all share share the ability to manipulate functions
during execution; that is, they are not limited to executing only the
function definitions specified by the programmer -- they can define
new functions as part of a computation.

%%\emph{What is the \lamA}

Alonzo Church defined his \lamA (``lambda calculus'') in 19XX
\citep{ChurchXX} to study systems of recursive equations. Being
Turing-complete, it can be used to model the behavior of any
computational system. However, it is particularly useful for modeling
functional programming languages. %% why?

Figure \ref{fig_lang1} gives a syntax for the ``pure''
\lamA. ``Abstraction'' defines a new function, while ``application''
gives a particular argument to a function. ``Variables'' are defined
when mentioned. 

\begin{myfig}[ht]
\begin{minipage}{3in}
\begin{Verbatim}
##                 ##      ##       
 #                  #       #       
 #  ###  ########   ###   ###  ###  
 #  ###   #  #  #   #  # #  #  ###  
 #  # #   #  #  #   #  # #  #  # #  
### ## # #########  ###   #### ## # 
\end{Verbatim}
\end{minipage}
  \caption{The \lamA' syntax.}
  \label{fig_lang1}
\end{myfig}

%%\emph{What does it look like?}

Using this syntax, we can define some common functions. \emph{Identity} 
returns its argument:
\begin{align}
  & \lamAbs{x}{x}. \\
\intertext{\emph{Compose} takes two functions and an argument. The result of
applying the second function to the argument is passed to the first:}
  & \lamAbs{f}{\lamAbs{g}{\lamAbs}{x}{\lamApp{f}{\lamPApp{g}{x}}}}. \\
\intertext{\emph{Const} takes two arguments but always returns the first:}
  & \lamAbs{f}{\lamAbs{a}{\lamAbs{b}{a}}}.
\end{align}

\begin{myfig}[bt]
\begin{minipage}{2in}
\begin{Verbatim}
                ##  
                 #  
 ##  ## ## ###   #  
####  # ## ###   #  
#     ###  # #   #  
 ###   #   ## # ### 
\end{Verbatim}
\end{minipage}
  \caption{Evaluation rules for \lamA. These rules show 
    \emph{call-by-value}, where arguments are evaluated
    before functions.}
  \label{fig_lang2}
\end{myfig}

A \lamA term executes by rewriting the expression according to the
rules in Figure \ref{fig_lang2}. We match our term to each of the
patterns above the line. If we have a match, we rewrite according to
the pattern below the line. When no more matches can be made, we say
the term is in \emph{normal form}: we have finished executing.

The rules given implement \emph{call-by-value} evaluation order,
meaning arguments to a function are evaluated before the function
itself. Other variants include \emph{call-by-need} and
\emph{call-by-name}, where arguments are not evaluated until
needed. We do not considers those variants further, however.

\section{Compiling the \LamA}

%% Define which steps in compilation we're going to worry about
Compiling even a language as simple as the \lamA involves a number of
steps, such as defining a concrete syntax, parsing source programs
into an \emph{abstract syntax tree} (AST), and producing an executable
program from the AST. For our purposes, however, we just focus on the
\lamA' three fundamental operations:

\begin{itemize}
\item Find a value by name (\emph{variables}).
\item Apply a function to an argument (\emph{application}).
\item Create a new function (\emph{abstraction}). 
\end{itemize}

Any compiler for the \lamA must be able to produce executable programs
which implement these operations. 

\subsection{The Target Machine}
We begin by defining a \emph{target machine}, |M|, for our compiler. To
reduce complexity we do not target an actual computer, but one of our
own design. Our machine will have an infinite number of
\emph{registers} (i.e., storage locations) that we can refer to by
name. It will have an unlimited supply of memory (called the
\emph{heap}) in which we can allocate structured values. However, we
will not refer to memory locations directly. Instead, we will always
store references to heap values in registers. Finally, the machine
will execute a list of instructions (our \emph{program}), starting at
the beginning and proceeding in sequential order (unless otherwise
instructed), until reaching the end of the list. Each instruction will
have a definite location, but we will only refer to certain special
locations using named labels.

\subsection{M's Language: \machLam}
Table \ref{tbl_lang1} gives the language that our machine will
execute, \machLam. A benefit of defining our own machine is that we
can also define the language it executes -- and the language we need
to compile to! We cannot make it too dissimilar from a ``real''
machine, but at this stage it helps to keep things simple. 

\begin{table}[th]
  \centering
  \begin{tabular}{lp{3.5in}}
    \emph{Instruction} & \emph{Description} \\
    \cmidrule(r){1-1}\cmidrule(r){2-2}
    \texttt{Store \emph{R} (\emph{F}, \emph{M})} & Store the value found in register #R# to field %%
    #F# of the value in register #M#. \\
    \texttt{Load (\emph{F}, \emph{M}) \emph{R}} & Load field #F# of the value in register #M# to register #R#. \\
    \texttt{Set \emph{v} \emph{R}} & Sets the register #R# to name of the variable $v$. \\
    \texttt{Copy \emph{R} \emph{M}} & Copies the contents of register #R# to register #M#. \\
    #Enter# & Jump to the location indicated by the closure in
    register #clo#, assuming an argument in register #arg#. The next #Return# executed
    will return to this location, with a result in register #res#.\\
    #Return# & Jump to the instruction following the most recently 
    executed #Enter# instruction and begin executing.  \\
    \texttt{MkClo \emph{L} [\emph{R}, \emph{S}, \dots]} &  Create a closure pointing to the 
    label #L# and holding the values in registers #R#, #S#, etc. The closure will be stored in 
    the #res# register.
  \end{tabular}
  \caption{\machLam, the ``machine language'' executed by our machine |M|.}
  \label{tbl_lang1}
  \figend
\end{table}

Each instruction supports an some aspect of the \lamA. In brief:
\begin{description}
\item[Variables] -- #Store# and #Load# help access variables and
  function arguments.
\item[Function Application] -- #Enter# and #Return# allow us to execute a function with arguments.
\item[Abstraction] -- #MkClo# lets us create functions as values.
\end{description}
The following sections describe each aspect in detail.

\subsection{Variables}

%% Free variables and environment
Consider how to find a value by its name. For example, in the
following program fragment
\begin{equation}
  \lamAbs{x}{\lamApp{f}{\lamPApp{g}{x}}}.
  \label{eq_lang1}
\end{equation}
we see three variables: $f$, $g$, and $x$. We say $x$ is \emph{bound},
because it is given as an argument, and that $f$ and $g$ are
\emph{free} because, in this context, they are not bound by a
$\lambda$-abstraction. To evaluate this expression, though, we need
a way to find the values of these terms.  

We can describe where to find $f$, $g$ and $x$ in terms of memory
locations. We can say that $x$ will appear in a special location,
$arg$, because it is the argument to the function and we will always
put arguments in the same place. We can further say that another
special location, $clo$, will have two
slots. The first will contain $g$ and the second will contain
$f$. Conceptually, then, our expression can be represented as:
\begin{center}
  \begin{tabular}{c}
    \begin{math}\begin{aligned}[b]
      arg &= x, \\
      clo[0] &= g, \\
      clo[1] &= f 
    \end{aligned}\text{\ in}\end{math} \\
    \lamAbs{arg}{\lamApp{clo[1]}{\lamPApp{clo[0]}{arg}}}.
  \end{tabular}
\end{center}

\par
In general, the $clo$ location holds the \emph{environment} for our
expression. For any given expression, we will be able to find all the
free variables (i.e., all those except the argument) in the
environment. The compiler will be responsible for ensuring the correct
environment is available whenever a given expression is evaluated.

Our machine, then, must have instructions for storing and retrieving
values. #Store# and #Load# (from Table \ref{tbl_lang1}) serve this
purpose. 

\subsection{Function Application}

%% Application & closures
Associating location with names is not enough, however. Looking again
at Equation \ref{eq_lang1}, it is clear $g$ represents a function, to
which we are passing the argument $x$. To compute the value of
$\lamPApp{g}{x}$, we must be able to execute the code representing
$g$. Because we have a storage location for $g$ already, we can
say that the value in $clo[0]$ holds a \emph{label} that tells us
where to find the code representing $g$. 

We now have two different types of values: an environment which tells
the currently evaluating function where to find its free variables;
and labels which tells us where to find the code representing a
function. However, $g$ may refer to any number of free variables, so
it makes sense to pair the label in $clo[0]$ with $g$'s
environment. We call this data structure a \emph{closure}. Closures
are the fundamental data structures used to compile functional
languages. They may not have the exact form described here but they
always have the same purpose: they pair a label with the free
variables used in the function represented.

#Enter# and #Return# (give in Table \ref{tbl_lang1}) implement
function application. #Enter# expects to find a closure in register
#clo# and will cause the machine to start executing the code at the
label given in the closure. A #Return# instruction will cause the
machine to jump back to the instruction following the most recently
executed #Enter# instruction. The machine will maintain a stack of
return locations so that #Enter# instructions can be nested.

We also use the #Copy# instruction to save and restore registers before
and after an #Enter# instruction. 

\subsection{Abstraction}
The \lamA lets us define functions which return new functions. We have
seen how to access variables in the environment and how to execute
unknown functions using closures. Now we come to the final element
needed to compile our \lamA to \machLam -- how to create
closures.

Consider the following expression, where we apply the |identity|
function to an argument:
\[\lamApp{(\lamAbs{x}{x})}{s}.\]
From the previous section, we know #Enter# and #Return# are used to
implement the application of \lamAbs{x}{x} to $s$. It follows that
\lamAbs{x}{x} must create a closure which #Enter# will then use to
execute the body of the $\lambda$-abstraction. In fact, Each
$\lambda$ in our source program returns a closure and that closure
points to code that implements the body of the $\lambda$ term.

The #MkClo# (from Table \ref{tbl_lang1}) instruction creates closures
for us. It takes a label, pointing to the code which will be executed
by the #Enter# instruction, and a list of registers, holding the free
variables found in the body of the expression.

\subsection{Compiling from \lamA to \machLam}

Table \ref{tbl_lang2} gives our algorithm to compile from \lamA to
\machLam. We present it in in four parts, \emph{a} - \emph{d},
corresponding to the syntax of \lamA terms given in Figure
\ref{fig_lang1}. The ``fat brackets,'' \compMach{t}, represent our
compiler, with the term being compiled given as the argument, $t$.
Each term compiles to a given sequence of instructions. We also assume
a function $\rho$, maintained by the compiler, that knows which
register holds a given variable.

%% Compilation rules ...
\afterpage{\clearpage{\input{machLamComp}}\clearpage}

Table \ref{tbl_lang2}, part \emph{a}, shows the compilation
scheme for variables. Variable refrences that are not used
in function application can only be the body of an expression, so we
just copy the variable's name to the #res#
register and return.

Function application, \lamPApp{f}{g}, is shown in part
\emph{b}. To apply a function, we must save the current #clo#
and #arg# registers. The compiler creates \emph{fresh} registers,
guaranteed to be unused anywhere else in the program, to store #clo#
and #arg#. We then use $\rho$ to find the registers holding $f$ and
$g$. Remember that $f$ will be a closure, while $g$ will be some
value. We copy those values into #clo# and #arg#. The #Enter#
instruction will execute the code pointed to by #clo#. When that
function returns, we restore #clo# and #arg# from the fresh registers
created earlier.

Abstractions, such as \lamAbs{x}{t}, return a closure pointing to the
code implementing $t$. Therefore, our compiler needs to generate code
that returns a closure, which in turn points to the code generated for
the body of the abstraction. To accomplish this, our compiler
recursively calls itself on the body. We get a label back, which is the
location of the just compiled code. In parts \emph{c} and \emph{d}
the expression $l = \compMach{\lamAbs{y}{t}}$ shows this
recursive call, and the label that results. That label can then be used in the 
closure returned by the abstraction.

We separate compilation of abstractions into two cases, depending if
the body is an abstraction or not. In the first case, as shown in part
\emph{c}, we begin by marking the location of this code with a new label,
#m#. We prepare to create a new closure by copying all values out of
the current closure into fresh registers. We then create a closure that
points to the body of our abstraction, contains all the values found
in the current closure, and ``captures'' our argument in the new
closure. 

For example, consider compiling this expression:

\begin{equation}
  \lamAbs{x}{\lamAbs{y}{\lamApp{f}{\lamPApp{y}{x}}}}. 
\end{equation}

$f$ and $x$ must be available when the body
\lamPApp{f}{\lamPApp{y}{x}} executes. Therefore, the closure returned
by \lamAbs{x}{(\dots)} must copy all values in the existing
closure as well as add the argument, $x$.

Part \emph{d} shows the code generated when the body of an abstraction
is \emph{not} another abstraction. We first mark the location of the
start of the body with a new label, #m#.  We then find the free
variables in the body, calling them $v_1, \dots, v_n$. This is a
compile-time operation, not something the program will do when
executing.  We assume that value of each free variables can be found
in the corresponding closure slot. For example, $v_0$ will be found in
$clo[0]$, $v_1$ in $clo[1]$, and so on. We also copy the $arg$
register to the corresponding register for our argument, as determined
by the $\rho$ function. Now that we have placed all variables in the
registers expected by our function, we generate the code for our body
and place it inline.


%% Essence of compiling lambda-calculus

%% Define our intermediate language

%% Show compilation scheme

%% \section{Source Language}

%% \emph{Defines11 a \lamA variant with some monadic effects, enough to
%%   illustrate interesting programs.}

%% \section{Monadic Intermediate Language}

%% %% What does the language support?

%% Our monadic language takes its inspiration from Haskell's @do@
%% notation. It is a pure functional language, making allocation of data
%% structures and closures explicit via monadic syntax. Functions in MIL
%% define computations which, when run, can affect heap memory. Figure
%% \ref{figMILDef} gives the syntax of the language.

%% %% TODO: Mention that v restricts the term to variables
%% %% only.

%% \begin{figure}[h]
%% \begin{code}
%%   defM := k {v1, ..., vN} v = k1 {v1, ..., vN, v} 
%%     | k {v1, ..., vN} v = b(v1, ..., vN, v)
%%     | b(v1, ..., vN) = bodyM
%%     | t <- k {}

%%   bodyM := do 
%%     stmtM1 
%%     ... 
%%     stmtMN 
%%     tailM

%%   stmtM := v <- tailM
%%     | case v of [alt1, ..., altN]


%%   tailM := return v
%%     | v1 @ v2
%%     | k {v1, ..., vN}
%%     | f(v1, ..., vN)
%%     | C v1 ... vN

%%   alt := C v1 ... vN -> b(v1, ..., vM) -- m <= n
%% \end{code}
%% \caption{Concrete syntax for our monadic intermediate language.}
%% \label{figMILDef}
%% \end{figure}

%% MIL programs consist of a series of definitions (@defM@). Each
%% definition can be any of the following.

%% \begin{description}
%%   \item[Closure-capturing] (@k {v1, ..., vN} v = k1 {v1, ..., vN, v}@) -- This function
%%     expects to find the variables @v1, ..., vN@ in its own closure. It constructs
%%     a new closure containing the existing variables plus the newly captured variable
%%     @v@. The new closure refers to @k1@, another closure-capturing function.
%%   \item[Block-calling] (@k {v1, ..., vN} v = b(v1, ..., vN, v)@) -- This function immediately
%%     jumps to block @b@ with arguments @v1, ..., vN@ and @v@. No closure value needs to
%%     be constructed. 
%%   \item[Function block] (@b(v1, ..., vN) = bodyM@) -- This function executes the statements
%%     in the body. 
%%   \item[Top-level] (@t <- k {}@) -- This special case ensures top-level definitions in the program
%%     can be accessed like any other function. The notation indicates that @t@ holds a closure
%%     structure, referring to the definition @k@. 
%% \end{description}

%% Notice that we can distinguish syntatically between functions that
%% merely create a closure (@k { ... }@) and those that do actual work
%% (@b(...)@). The body of a @k@ functin can only allocate another
%% closure or jump to a block. A block, on the other hand, can do other
%% work, but it cannot directly return a closure. As will be described in
%% chapter \ref{ref_chapter_uncurrying} this makes it much easier to
%% recognize and elminate intermediate closures.

%% The body of each block consists of statements followed by a
%% \emph{tail}. Tails can only
%% appear as the last statement in a block or on the right-hand side of
%% the monadic arrow (``@<-@''). Tail instructions, in other words, cause 
%% effects. The three tail statements follow:

%% \begin{description}
%% \item[Return a computation] (@return v@) -- Returns the result of a computation
%%   to the caller.

%% \item[Create a closure] (@k {v1, ..., vN}@) -- Creates a closure pointing to
%%   function @k@, capturing variables @v1@ through @vN@.

%% \item[Enter a function] (@v1 @@ v2@) -- Enter the closure referred to by @v1@, with
%%   argument @v2@. In other words, function application. Note that @v1@ represents an
%%   \emph{unknown} function -- one for which we compute the address at run-time.

%% \item[Call a block] (@f(v1, ..., vN)@) -- Jump to the block labeled @f@ with the arguments
%%   given. In this case we know the function @f@ refers to and do not need to examine
%%   a closure in order to execute it.
%% \item[Create a value] (@C v1 ... vN@) -- Create a data value with tag @C@, holding
%%   the values found in variables @v1 ... vN@.
%% \end{description}

%% %% TODO: Describe alt syntax.

%% Statements in a block either bind the result of a tail statement 
%% (@v <- tailM@) or branch conditionally (@case v of ... @). Binding ``runs''
%% a computation and ``dereferences'' the result, placing
%% the value in a variable (e.g., @v@). That same variable can be bound
%% again later, but that does not affect previous uses of @v@. In essence, the old
%% name becomes hidden and its value inaccessible.

%% Though the syntax allows multiple @case@ statements in a function
%% body, only one can appear and it must be the last statement in the
%% body. The arms of the @case@ statement can only match on constructor
%% tags (@C@) and can only bind the constructor arguments to variables
%% (@v1 ... vN@). Each arm then jumps to a known block with those
%% variables as arguments. This choice makes compilation simpler.


%% %% \emph{Defines our monadic language and explains the terms in
%% %%   it. Example programs are given which illustrate closure construction
%% %%   and data allocation. The use of ``tail'' vs. statements is motivated
%% %%   and described. }

%% \emph{Need to talk about the monad we work in as well - what 
%% do bind and return mean?}

%% \section{Compiling to Our MIL}
%% \emph{A compilation scheme which uses Hoopls ``shapes'' is
%% described. This scheme will give use our initial, unoptimized
%% MIL program. An example (possibly |compose|, or |const3|) illustrates 
%% our scheme.}

\end{document}
