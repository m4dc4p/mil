\documentclass[12pt]{report}
%include polycode.fmt
\include{preamble}
\begin{document}
\include{document.preamble}

\chapter{Functional Languages}
\label{ref_chapter_languages}

%% \emph{Brief definitino of ``functional languages.'' Introduce our
%%   lambda calculus variant. Shows a compilation scheme from the variant
%%   to Caffeine's RTL as an example}

\section{The \LamA}
%%\emph{Why is it important}

In mathematics, a ``function'' takes one or more arguments and
produces some value. ``Functional programming languages'' are based
around the same idea -- the definition and evaluation of
functions. They all share share the ability to manipulate functions
during execution; that is, they are not limited to executing only the
function definitions specified by the programmer -- they can define
new functions as part of a computation.

%%\emph{What is the \lamA}

Alonzo Church defined his \lamA (``lambda calculus'') in 19XX
\citep{ChurchXX} to study systems of recursive equations. Being
Turing-complete, it can be used to model the behavior of any
computational system. However, it is particularly useful for modeling
functional programming languages. %% why?

Figure \ref{fig_lang1} gives a syntax for the ``pure''
\lamA. ``Abstraction'' defines a new function, while ``application''
gives a particular argument to a function. ``Variables'' are defined
when mentioned. 

\begin{myfig}[ht]
\begin{minipage}{3in}
\begin{Verbatim}
##                 ##      ##       
 #                  #       #       
 #  ###  ########   ###   ###  ###  
 #  ###   #  #  #   #  # #  #  ###  
 #  # #   #  #  #   #  # #  #  # #  
### ## # #########  ###   #### ## # 
\end{Verbatim}
\end{minipage}
  \caption{The \lamA' syntax.}
  \label{fig_lang1}
\end{myfig}

%%\emph{What does it look like?}

Using this syntax, we can define some common functions. \emph{Identity} 
returns its argument:
\begin{align}
  & \lamAbs{x}{x}. \\
\intertext{\emph{Compose} takes two functions and an argument. The result of
applying the second function to the argument is passed to the first:}
  & \lamAbs{f}{\lamAbs{g}{\lamAbs}{x}{\lamApp{f}{\lamPApp{g}{x}}}}. \\
\intertext{\emph{Const} takes two arguments but always returns the first:}
  & \lamAbs{f}{\lamAbs{a}{\lamAbs{b}{a}}}.
\end{align}

\begin{myfig}[bt]
\begin{minipage}{2in}
\begin{Verbatim}
                ##  
                 #  
 ##  ## ## ###   #  
####  # ## ###   #  
#     ###  # #   #  
 ###   #   ## # ### 
\end{Verbatim}
\end{minipage}
  \caption{Evaluation rules for \lamA. These rules show 
    \emph{call-by-value}, where arguments are evaluated
    before functions.}
  \label{fig_lang2}
\end{myfig}

A \lamA term executes by rewriting the expression according to the
rules in Figure \ref{fig_lang2}. We match our term to each of the
patterns above the line. If we have a match, we rewrite according to
the pattern below the line. When no more matches can be made, we say
the term is in \emph{normal form}: we have finished executing.

The rules given implement \emph{call-by-value} evaluation order,
meaning arguments to a function are evaluated before the function
itself. Other variants include \emph{call-by-need} and
\emph{call-by-name}, where arguments are not evaluated until
needed. We do not considers those variants further, however.

%\emph{A few sample programs in \lamA -- natural numbers \& arithmetic, booleans
%\& logic functions}

%%%
%%% Presentation below due to Pierce ``Types and Programming Languages.''
%%%

%% This form is the simplest version of the \lamA, and it
%% does not much resemble a programming language. However, we can 
%% quickly build some interesting programs with it. Consider the 
%% natural numbers (0, 1, 2, \ellipsis, etc.). We can define 0 as
%% the function that just returns its argument, $z$:
%% \begin{align}
%%   0 &= \lamAbs{s}{\lamAbs{z}{z}}. \\
%%   \intertext{Similarly, we can define 1 as the function that returns $s$ applied
%%     to $z$ once:}
%%   1 &= \lamAbs{s}{\lamAbs{z}{\lamApp{s}{z}}}. \\
%%     \intertext{In fact, all the naturals can be defined in the same way. The next few are:}
%%     2 &= \lamAbs{s}{\lamAbs{z}{\lamApp{s}{\lamPApp{s}{z}}}}. \\
%%     3 &= {\lamAbs{s}{\lamAbs{z}{\lamApp{s}{\lamPApp{s}{\lamPApp{s}{z}}}}}}}.
%%     \intertext{}
%% \end{align}

%% We can interpret the number of $s$' as the natural represented by
%% these functions. Given that, we can define arithmetic functions
%% in terms of $s$ and $z$.  $succ$, the successor function, 
%% which always returns the successor the the natural given:
%% \begin{align}
%%   succ &= \lamAbs{n}{\lamAbs{s}{\lamAbs{z}{\lamApp{s}{\lamPApp{n}{\lamPApp{s}{z}}}}}}. 
%%   \intertext{We can find the sum of two numbers with $add$:}
%%   add &= \lamAbs{m}{\lamAbs{n}{\lamAbs{s}{\lamAbs{z}{\lamApp{m}{\lamPApp{s}{n}}}}}.
%%   \intertext{We multiply by adding $m$ to $n$ over and over, $m$ times:}
%%   mult &= \lamAbs{m}{\lamAbs{n}{\lamApp{m}{\lamPApp{\lamPApp{add}{n}}}{m}}}.
%% \end{align}

\section{Compiling the \LamA}

%% Define which steps in compilation we're going to worry about
Compiling even a language as simple as the \lamA involves a number of
steps, such as defining a concrete syntax, parsing source programs
into an \emph{abstract syntax tree} (AST), and producing an executable
program from the AST. For our purposes, however, we just focus on the
\lamA' three fundamental operations:
\begin{description}
\item Find a value by name(\emph{variables}).
\item Apply a function to an argument (\emph{application}).
\item Create a new function (\emph{abstraction}). 
\end{description}
Any compiler for the \lamA must be able to produce executable programs
which implement these operations. 

\subsection{The Target Machine}
We begin by defining a \emph{target machine}, $M_\lambda$, for our compiler. To
reduce complexity we do not target an actual computer, but one of our
own design. Our machine will have an infinite number of
\emph{registers} (i.e., storage locations) that we can refer to by
name. It will have an unlimited supply of memory (called the
\emph{heap}) in which we can allocate structured values. However, we
will not refer to memory locations directly. Instead, we will always
store references to heap values in registers. Finally, the machine
will execute a list of instructions (our \emph{program}), starting at
the beginning and proceeding in sequential order (unless otherwise
instructed), until reaching the end of the list. Each instruction will
have a definite location, but we will only refer to certain special
locations using named labels.

A benefit of defining our own machine is that we can also define the
language it executes -- and the language we need to compile to! We
cannot make it too dissimimlar from a ``real'' machine, but at this
stage it helps to keep things simple. 

\subsection{Variables}

%% Free variables and environment
Before discussing our compiler, let us first consider how to find
a value by its name. For example, in the following program fragment
\begin{equation}
  \lamAbs{x}{\lamApp{f}{\lamPApp{g}{x}}}.
  \label{eq_lang1}
\end{equation}
we see three variables: $f$, $g$, and $x$. We say $x$ is \emph{bound},
because it is given as an argument, and that $f$ and $g$ are
\emph{free} because, in this context, they are not bound by a
$\lambda$-abstraction. To evaluate this expression, though, we need
a way to find the values of these terms.  

We can describe where to find $f$, $g$ and $x$ in terms of memory
locations. We can say that $x$ will appear in a special location,
$arg$, because it is the argument to the function and we will always
put arguments in the same place. We can further say that another
special location, $clo$, will have two
slots. The first will contain $g$ and the second will contain
$f$. Conceptually, then, our expression can be represented as:
\begin{equation}
  \begin{split}
    arg &= x, \\
    clo[0] &= g, \\
    clo[1] &= f \text{in} \\
    \lamAbs{_}{\lamApp{clo[1]}{\lamPApp{clo[0]}{arg}}}.
  \end{split}
\end{equation}

In general, the $clo$ location holds the \emph{environment} for our
expression. For any given expression, we will be able to find all the
free variables (i.e., all those except the argument) in the
environment. The compiler will be responsible for ensuring the correct
environment is available whenever a given expression is evaluated.

Our machine, then, must have instructions for storing and retrieving
values. We define two instructions for this purpose:
\begin{center}
  \begin{tabular}{ll}
    #Store R (F, M)# & Store the value found in register #R# to field %%
    #F# of the value in register #M#. \\
    #Load (F, M) R# & Load field #F# of the value in register #M# to register #R#. \\
  \end{tabular}
\end{center}

#R#, the register name, can be anything, including ``arg'' and ``clo'', for the special locations named above.

\subsection{Function Application}

%% Application & closures
Associating location with names is not enough, however. Looking again
at Equation \ref{eq_lang1}, it is clear $g$ represents a function, to
which we are passing the argument $x$. To compute the value of
$\lamPApp{g}{x}$, we must be able to execute the code representing
$g$. Because we have a storage location for $g$ already, we can
say that the value in $clo[0]$ holds a \emph{label} that tells us
where to find the code representing $g$. 

We now have two different types of values: an environment which tells
the currently evaluating function where to find its free variables;
and labels which tells us where to find the code representing a
function. However, $g$ may refer to any number of free variables, so
it makes sense to pair the label in $clo[0]$ with $g$'s environment. We call
this data structure a \emph{closure}.

Closures are the fundamental data structures used to compile functional
languages. They may not have the exact form described here but they always
have the same purpose: they pair a label with the free variables used in the
function represented. 

\subsection{Abstraction}

%% Essence of compiling lambda-calculus

%% Define our intermediate language

%% Show compilation scheme

%% \section{Source Language}

%% \emph{Defines11 a \lamA variant with some monadic effects, enough to
%%   illustrate interesting programs.}

%% \section{Monadic Intermediate Language}

%% %% What does the language support?

%% Our monadic language takes its inspiration from Haskell's @do@
%% notation. It is a pure functional language, making allocation of data
%% structures and closures explicit via monadic syntax. Functions in MIL
%% define computations which, when run, can affect heap memory. Figure
%% \ref{figMILDef} gives the syntax of the language.

%% %% TODO: Mention that v restricts the term to variables
%% %% only.

%% \begin{figure}[h]
%% \begin{code}
%%   defM := k {v1, ..., vN} v = k1 {v1, ..., vN, v} 
%%     | k {v1, ..., vN} v = b(v1, ..., vN, v)
%%     | b(v1, ..., vN) = bodyM
%%     | t <- k {}

%%   bodyM := do 
%%     stmtM1 
%%     ... 
%%     stmtMN 
%%     tailM

%%   stmtM := v <- tailM
%%     | case v of [alt1, ..., altN]


%%   tailM := return v
%%     | v1 @ v2
%%     | k {v1, ..., vN}
%%     | f(v1, ..., vN)
%%     | C v1 ... vN

%%   alt := C v1 ... vN -> b(v1, ..., vM) -- m <= n
%% \end{code}
%% \caption{Concrete syntax for our monadic intermediate language.}
%% \label{figMILDef}
%% \end{figure}

%% MIL programs consist of a series of definitions (@defM@). Each
%% definition can be any of the following.

%% \begin{description}
%%   \item[Closure-capturing] (@k {v1, ..., vN} v = k1 {v1, ..., vN, v}@) -- This function
%%     expects to find the variables @v1, ..., vN@ in its own closure. It constructs
%%     a new closure containing the existing variables plus the newly captured variable
%%     @v@. The new closure refers to @k1@, another closure-capturing function.
%%   \item[Block-calling] (@k {v1, ..., vN} v = b(v1, ..., vN, v)@) -- This function immediately
%%     jumps to block @b@ with arguments @v1, ..., vN@ and @v@. No closure value needs to
%%     be constructed. 
%%   \item[Function block] (@b(v1, ..., vN) = bodyM@) -- This function executes the statements
%%     in the body. 
%%   \item[Top-level] (@t <- k {}@) -- This special case ensures top-level definitions in the program
%%     can be accessed like any other function. The notation indicates that @t@ holds a closure
%%     structure, referring to the definition @k@. 
%% \end{description}

%% Notice that we can distinguish syntatically between functions that
%% merely create a closure (@k { ... }@) and those that do actual work
%% (@b(...)@). The body of a @k@ functin can only allocate another
%% closure or jump to a block. A block, on the other hand, can do other
%% work, but it cannot directly return a closure. As will be described in
%% chapter \ref{ref_chapter_uncurrying} this makes it much easier to
%% recognize and elminate intermediate closures.

%% The body of each block consists of statements followed by a
%% \emph{tail}. Tails can only
%% appear as the last statement in a block or on the right-hand side of
%% the monadic arrow (``@<-@''). Tail instructions, in other words, cause 
%% effects. The three tail statements follow:

%% \begin{description}
%% \item[Return a computation] (@return v@) -- Returns the result of a computation
%%   to the caller.

%% \item[Create a closure] (@k {v1, ..., vN}@) -- Creates a closure pointing to
%%   function @k@, capturing variables @v1@ through @vN@.

%% \item[Enter a function] (@v1 @@ v2@) -- Enter the closure referred to by @v1@, with
%%   argument @v2@. In other words, function application. Note that @v1@ represents an
%%   \emph{unknown} function -- one for which we compute the address at run-time.

%% \item[Call a block] (@f(v1, ..., vN)@) -- Jump to the block labeled @f@ with the arguments
%%   given. In this case we know the function @f@ refers to and do not need to examine
%%   a closure in order to execute it.
%% \item[Create a value] (@C v1 ... vN@) -- Create a data value with tag @C@, holding
%%   the values found in variables @v1 ... vN@.
%% \end{description}

%% %% TODO: Describe alt syntax.

%% Statements in a block either bind the result of a tail statement 
%% (@v <- tailM@) or branch conditionally (@case v of ... @). Binding ``runs''
%% a computation and ``dereferences'' the result, placing
%% the value in a variable (e.g., @v@). That same variable can be bound
%% again later, but that does not affect previous uses of @v@. In essence, the old
%% name becomes hidden and its value inaccessible.

%% Though the syntax allows multiple @case@ statements in a function
%% body, only one can appear and it must be the last statement in the
%% body. The arms of the @case@ statement can only match on constructor
%% tags (@C@) and can only bind the constructor arguments to variables
%% (@v1 ... vN@). Each arm then jumps to a known block with those
%% variables as arguments. This choice makes compilation simpler.


%% %% \emph{Defines our monadic language and explains the terms in
%% %%   it. Example programs are given which illustrate closure construction
%% %%   and data allocation. The use of ``tail'' vs. statements is motivated
%% %%   and described. }

%% \emph{Need to talk about the monad we work in as well - what 
%% do bind and return mean?}

%% \section{Compiling to Our MIL}
%% \emph{A compilation scheme which uses Hoopls ``shapes'' is
%% described. This scheme will give use our initial, unoptimized
%% MIL program. An example (possibly |compose|, or |const3|) illustrates 
%% our scheme.}

\end{document}
