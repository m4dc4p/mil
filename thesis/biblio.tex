\documentclass[11pt]{article}
\usepackage{bibentry, natbib}
\usepackage{palatino}
\usepackage[headings]{fullpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\begin{document}
\bibliographystyle{abbrvnat}
\nobibliography{biblio}

\bibentry{Kennedy07Compiling}
\bigskip

Kennedy contrasts an earlier implemention of his SML.NET compiler that
used a ``monadic intermediate language'' (MIL) with a new
implementation that uses continuation-passing style. He highlights
three problems with the MIL approach:

\begin{enumerate}
\item ``Administrative'' reduction rules must be defined. He uses
  \textsc{CC-Let}, the associativity of let statements, to illustrate
  one of these rules.
\item The order of reductions can significantly affect performance. He
  gives an example which can cause $O(n)$ reductions one way, and
  $O(n^2)$ another.
\item ``Join point'' functions -- when a case construct is lifted out
  of a let, terms can be duplicated. A ``Join point'' function is
  usually introduced to avoid duplication. These functions can be hard
  to eliminate.
\end{enumerate}

On the third point, will introducing ``k'' functions avoid creating
explicit ``join point'' functions?

This paper has several good examples in the section that contrasts MIL
with CPS. They are clearly designed to make CPS look good so being
able to compile them efficiently would be a nice result.

In related work, he mentions the Sequential Intermediate Language described by
\citet{TOLMACH98FromML}. Worth looking into.

\bigskip
\bibentry{TOLMACH98FromML}
\bigskip

Section 10 (``Optimization of first-order code'') describes several
interesting closure analysis techniques. Section 10.2 (``Implicit
type-based closure analysis'') discusses how they use type-inference
to narrow the functions that might be invoked at each call site. Only
functions that match the types required will be called. In many cases
this lets them narrow the possible functions to a small number,
sometimes even one. Those sites become candidates for explicit jumps
rather than indirect calls through a pointer. Section 10.3 (``Explicit
closure analysis'') does not describe a technique that directly
eliminates closures or indirection. However, the analysis seems to
make it easier for later stages to eliminate indirection. I was not as
clear on the benefit of this technique, but it seems worth
investigating.

Section 12.2 (``Algebraic type representation'') talks about ways to
avoid representing values in the heap, and discusses the tradeoffs
involved. They say they do the ``standard stuff'' and reference two
other papers for details: \emph{Compiling with Continuations}
\citep{Appel92Compiling} and ``Compiling a Functional Language''
\citep{Cardelli84Compiling}.

\bigskip
\bibentry{Morris73Advice}
\bigskip

Unclear the relevance of this paper. Not studied close enough.

\begin{quote}
``The purpose of this paper is to advise an approach (and to support
that advice by discussion of an example) towards achieving a goal
first announced by John McCarthy: that compilers for higher-level
programming languages should be made completely trustworthy by proving
their correctness. The author believes that the compiler-correctness
Problem can be made much less general and better-structured than the
unrestricted program-correctness problem;''
\end{quote}

\begin{itemize}
\item ``More Advice on Proving a Compiler Correct: Improve a Correct Compiler''
Erik Meijer
\end{itemize}

Compilers are derived from denotational semantics. Interpreters are used to
derive the compiler. A new compiler is derived from each previous compiler, yielding
more efficient programs. 

Uses ``aspects of Action Semantics'' by defining the compiler as a collection of
``semantic functions'' from the source language to the ``initial action algebra.''

This paper is a condensed version of his Ph.D. thesis.

\bibentry{Ohori07ProofTheory}

Ohori develops a proof system for modeling low level code
languages. He establishes a Curry-Howard correspondance between the
typed lambda calculus and his proof system. The proof system can represent
closures, jumps (goto) and loops, though you lose termination. However, type-soundness
can be proven. 

It seems that if a machine language can be modeled in his proof
system, then all the properties of that proof system apply.

\paragraph{Open questions}

\begin{itemize}
\item What is ``cut elimination''?
\item How does the introduction of specific machine instructions and states work? How do more
  complex operational machine states affect the theory?
\end{itemize}

\bibentry{Higuchi02JavaBytecode}

This paper is an example of using the theory from ``A Formal Theory of
Machine Code'', though it is based on the earlier ``Logical Abstract
Machine'' work. It provides a type system for JVM
bytecodes. Operational semantics for the JVM are given, and the type
system is proved sound and correct. Does not address type-preserving
compilation, even though it is mentioned in the abstract. Does give a
type-inference algorithm for Java programs, based on bytecode types.

The conclusion implies that future work will report on other
developments, which may be worth pursuing.

\bibentry{Wadler90ComprehendingMonads}

A long paper which introduces ``monad comprehensions,'' and shows
their relation to list comprehensions. He shows how an ``impure''
functional language with assignment can be translated intoa pure
language with a State monad. Section 8 shows how any program, in the
impure functional language, can be translated into one in the pure
language. He shows how well-typed terms in the source language are
preserved in the target language.

The paper claims to describe a unique approach for safely updating an
array in-place, using a state monad and a reader monad (he calls them
``state-transforming'' and ``state-reader'' monads). The basis for
state threads by Peyton Jones and Launchbury?

Examples of several monads are given, and category theory concepts are
introduced as needed.

\bibentry{Reynolds98definitionalinterpreters}

A retrospective on ``Definitional Interpreters for Higher-Order
Languages.'' Most interesting is the reference to ``closure
conversion'' (cited below).

\bibentry{Minamide96typedclosure}

Worth looking into -- describes ``the replacement of lambda
expressions by closure constructors'' in a typed setting.

\bibentry{Elliot09BeautifulDifferentiation}

Most interesting is his reference to ``type morphisms'' as a way to
construct correct programs. He references ``Denotational design with
type class morphisms'' at
http://conal.net/papers/type-class-morphisms.

\bibentry{Baars09TypedTransformations}

This paper discusses ways to transform an embedded domain specific
language, with attached phantom types. They describe a library for
transforming ``hosted grammers'' -- could be worth pursuing more.

\bibentry{Chlipala10SyntaticProofs}

Found at \url{http://ltamer.sourceforge.net/}, this submitted draft's abstract is:

\begin{quote}
``Semantic preservation by compilers for higher-order languages can be
  veriﬁed using simple syntactic methods. At the heart of classic
  techniques are relations between source-level and target-level
  values. Unfortunately, these relations are speciﬁc to particular
  compilers, leading to correctness theorems that have nothing to say
  about linking programs with functions compiled by other compilers or
  written by hand in the target language. Theorems based on logical
  relations manage to avoid this problem, but at a cost: standard
  logical relations do not apply directly to programs with
  non-termination or impurity, and extensions to handle those features
  are relatively complicated, compared to the classical compiler
  veriﬁcation literature.

  In this paper, we present a new approach to ``open'' compiler
  correctness theorems that is ``syntactic'' in the sense that the
  core relations do not refer to semantics. Though the technique is
  much more elementary than previous proposals, it scales up nicely
  to realistic languages. In particular, untyped and impure programs
  may be handled simply, while previous work has addressed neither
  in this context.
  
  Our approach is based on the observation that it is an unnecessary
  handicap to consider proofs as black boxes. We identify some
  theorem-speciﬁc proof skeletons, such that we can deﬁne an algebra
  of nondeterministic compilations and their proofs, and we can
  compose any two compilations to produce a correct-by-construction
  result. We have prototyped these ideas with a Coq implementation
  of multiple CPS translations for an untyped Mini-ML source
  language with recursive functions, sums, products, mutable
  references, and exceptions. ''
\end{quote}

\bibentry{Chlipala10CertifiedProgramming}

A book on programming in Coq. Might be useful for exploring alternate
approaches to correctness.


\bibentry{Felleisen08TypedScheme}

Describes a type system which allows typed and untyped Scheme programs
to run together. The ``occurrence'' typing is novel, but the
sub-typing shown might be more useful, as it allows types to be refined
during inference.

Also references the ``typecase'' keyword, from work by Cartwright
(``User defined data types as an aid to verifying Lisp programs''), which may
prove ``seminal.''

\bibentry{Launchbury94Lazy}

They describe how stateful computations can be hidden inside a ``state
transformer'' and made to appear pure outside the transformer. Used to
implement mutable array updates and I/O sequencing. Seems it could be
used to implement heap allocation (i.e., closure construction).

Need to look up citations though -- who else has used their result?

\bibentry{Myreen10Verified}

Presents a just-in-time compiler for a simple language that 
produces verified machine code. Interesting for results on:

\begin{itemize}
\item Semantics for x86 instructions are described.
\item The code generator, which translates from bytecode to machine code,
is verified,
\item The author alludes to work in their thesis (\cite{Myreen09Formal}) which
decompiles machine code into functions.
\item A number of references are interesting: a compiler verification
  bibliography (\cite{Dave03Compiler}), previous work on compiler verification (\cite{Myreen09Extensible},
  \cite{Myreen08Machine}), and possibly the paper introducing Hoare triples/logic (\cite{Hoare69Axiomatic}).
 
\end{itemize}

\bibentry{Chlipala07Certified}

Haven't read yet but very interesting title.

\end{document}
